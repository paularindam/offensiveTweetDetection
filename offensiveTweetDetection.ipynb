{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import codecs,os,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score,cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit,train_test_split \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.svm import LinearSVC,NuSVC\n",
    "from sklearn.linear_model import SGDClassifier ,PassiveAggressiveClassifier,Perceptron,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier,VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB,GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Normalizer,FunctionTransformer,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,Callback\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "from keras.layers import Dense,Activation,Dropout, LSTM, Bidirectional,TimeDistributed,GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence,text as Text\n",
    "\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.utils import vis_utils #import plot_model, model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model):\n",
    "    return SVG(vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test,y_pred):\n",
    "    print \n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test,y_pred)*100))\n",
    "    print(\"Precision: %.2f%%\" % (precision_score(y_test,y_pred)*100))\n",
    "    print(\"Recall: %.2f%%\" % (recall_score(y_test,y_pred)*100))\n",
    "    print(\"F1-score: %.2f%%\" % (f1_score(y_test,y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_patience = [EarlyStopping(monitor='val_loss',patience=2)]\n",
    "earlystopping_littlepatience = [EarlyStopping(monitor='val_loss',patience=1)]\n",
    "earlystopping = earlystopping_nopatience = [EarlyStopping(monitor='val_loss',patience=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list)-ngram_range+1):\n",
    "            for ngram_value in range(2, ngram_range+1):\n",
    "                ngram = tuple(new_list[i:i+ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "ngram_range = 2\n",
    "print('Adding {}-gram features'.format(ngram_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning utf-8 encoding issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(texts):\n",
    "    cleanedTexts = []\n",
    "    for text in texts:\n",
    "        cleanedTexts += [unicode(str(text),errors='ignore').encode(\"latin-1\",\"ignore\").replace('\\r','')]\n",
    "    return cleanedTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse word order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverseText(text):\n",
    "    words=text.split()\n",
    "    # reverse list\n",
    "    words.reverse()\n",
    "    # now print\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Maximum length of a tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(texts=tweets):\n",
    "    maxLength = -1\n",
    "    for text in texts:\n",
    "        if len(text)>maxLength:\n",
    "            maxLength = len(text)\n",
    "    return maxLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting test-train loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTestTrainCurves(history):\n",
    "    print \"History Summary for accuracy\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    print \"History Summary for loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_split(X=tweets,y=labels):\n",
    "    X = array(X)\n",
    "    y = array(y)\n",
    "    ###################################\n",
    "    X,y = shuffle(X,y,random_state=10)\n",
    "    X,y = shuffle(X,y,random_state=0)\n",
    "    ###################################\n",
    "    tk = Text.Tokenizer(lower=True)\n",
    "    tk.fit_on_texts(X)\n",
    "\n",
    "    x = tk.texts_to_sequences(X)\n",
    "\n",
    "    ###################################\n",
    "    max_len = 174\n",
    "    embedding_vecor_length = 32\n",
    "    print \"max_len \", max_len\n",
    "    print('Pad sequences (samples x time)')\n",
    "\n",
    "    x = sequence.pad_sequences(x, maxlen=max_len)\n",
    "\n",
    "    return train_test_split(x,y, test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> Embedding Models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove/glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    gloveW2VWiki = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove/glove.twitter.27B.50d.txt\", \"rb\") as lines:\n",
    "    gloveW2VTwitter = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove/glove.42B.300d.txt\", \"rb\") as lines:\n",
    "    gloveW2VCommonCrawlSmall = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove/glove.840B.300d.txt\", \"rb\") as lines:\n",
    "    gloveW2VCommonCrawlBig = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Words(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        tokens =  word_tokenize(sentence)\n",
    "        words +=[tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetCorpus = cleanTweets(open('profane_tweets.txt').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_transformer = Words(tweetCorpus)\n",
    "bigram_transformer = Phrases(tweetCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_folder = \"embeddingModels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_word_model = Word2Vec(unigram_transformer, size=100)\n",
    "CBOW_word_W2V = dict(zip(cbow_word_model.wv.index2word, cbow_word_model.wv.syn0))\n",
    "cbow_word_model.save(embedding_folder + 'profanity_CBOW.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_phrase_model = Word2Vec(bigram_transformer[tweetCorpus], size=100)\n",
    "CBOW_phrase_W2V = dict(zip(cbow_phrase_model.wv.index2word, cbow_phrase_model.wv.syn0))\n",
    "cbow_phrase_model.save(embedding_folder + 'profanity_CBOW.phrase2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_word_model = Word2Vec(unigram_transformer, sg=1, size=100)\n",
    "skipgram_word_W2V = dict(zip(skipgram_word_model.wv.index2word, skipgram_word_model.wv.syn0))\n",
    "skipgram_word_model.save(embedding_folder + 'profanity_Skipgram.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_phrase_model = Word2Vec(bigram_transformer[tweetCorpus], sg=1, size=100)\n",
    "skipgram_phrase_W2V = dict(zip(skipgram_phrase_model.wv.index2word, skipgram_phrase_model.wv.syn0))\n",
    "skipgram_phrase_model.save(embedding_folder + 'profanity_Skipgram.phrase2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEmbeddingMatrix(word_vectors, embedding, word_index, vocabulary_size=vocab_size):\n",
    "    NUM_WORDS = vocabulary_size\n",
    "    embedding_dim = len(embedding[embedding.keys()[0]])\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i>=NUM_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
    "    return embedding_matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for line in open('glove/glove.twitter.27B.50d.txt'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "glove_twitter_embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_twitter_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for line in open('glove/glove.twitter.27B.200d.txt'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "glove_twitterBig_embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_twitterBig_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Pretrain Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "NUM_WORDS = vocab_size\n",
    "google_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        google_embedding_vector = google_pretrained_vectors[word]\n",
    "        google_embedding_matrix[i] = google_embedding_vector\n",
    "    except KeyError:\n",
    "        google_embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(google_pretrained_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"teal\">Pretrain Embedding Models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_profane_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier( classifier='sgd',ngram=2,idf=True, X = tweets, y=labels):\n",
    "    start_time = time.time()\n",
    "    vectorizer = TfidfVectorizer(input='content', min_df=3,strip_accents='unicode', analyzer ='word', tokenizer=LemmaTokenizer(), ngram_range=(1, ngram), use_idf=idf, sublinear_tf=1, stop_words = \"english\")\n",
    "    clf = None\n",
    "    if classifier ==\"votingBestHard\":\n",
    "        clf =  Pipeline([('vect', vectorizer), VotingClassifier(estimators=[ ('pa', PassiveAggressiveClassifier()),('et', ExtraTreesClassifier(n_estimators=200)),('rf', RandomForestClassifier())])])\n",
    "        pass\n",
    "    elif classifier == \"votingBestSoft\":\n",
    "        clf = VotingClassifier(estimators=[])\n",
    "        pass\n",
    "    elif classifier=='naive':\n",
    "        clf =  Pipeline([('vect', vectorizer), ('nb', MultinomialNB())])\n",
    "    elif '3nn' in classifier:\n",
    "        clf =  Pipeline([('vect', vectorizer),('3nn',KNeighborsClassifier(n_neighbors=3))])\n",
    "    elif '5nn' in classifier:\n",
    "        clf =  Pipeline([('vect', vectorizer),('5nn',KNeighborsClassifier(n_neighbors=5))])\n",
    "    elif 'logistic' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('logistic', LogisticRegression(solver='newton-cg'))])\n",
    "    elif 'perceptron' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('perceptron', Perceptron())])\n",
    "    elif 'passiveaggressive' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('pa', PassiveAggressiveClassifier())])\n",
    "    elif 'paGlove' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2V)), ('pa', PassiveAggressiveClassifier())])\n",
    "    elif 'paGensim' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(bigramW2V)), ('pa', PassiveAggressiveClassifier())])\n",
    "    elif 'paSkipgram' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(skipBigramW2V)), ('pa', PassiveAggressiveClassifier())])\n",
    "    elif 'bernoulliTfidf' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('bnb', BernoulliNB(binarize=0.0))])\n",
    "    elif 'bernoulliGlove' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VWiki)), ('bnb', BernoulliNB(binarize=0.0))])\n",
    "    elif 'bernoulliGensim' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(bigramW2V)), ('bnb', BernoulliNB(binarize=0.0))])\n",
    "    elif 'svcTfidf' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('svc', LinearSVC())])\n",
    "    elif 'svcGlove' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VWiki)), ('svc', LinearSVC())])\n",
    "    elif 'svcGensim' in classifier:\n",
    "        clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(bigramW2V)), ('svc', LinearSVC())])\n",
    "    elif 'randomforest' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('rf', RandomForestClassifier(n_jobs=-1))])\n",
    "    elif 'extraTrees' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('et', ExtraTreesClassifier(n_estimators=200))])\n",
    "    elif 'lsa' in classifier:\n",
    "        clf = Pipeline([('countvect',CountVectorizer(min_df = 1, stop_words = 'english')),('svd',TruncatedSVD(2, algorithm = 'arpack')),('norm',Normalizer(copy=False)),('et', ExtraTreesClassifier(n_estimators=200))])\n",
    "    elif 'lsaRandom' in classifier:\n",
    "        clf = Pipeline([('countvect',CountVectorizer(min_df = 1, stop_words = 'english')),('svd',TruncatedSVD(2, algorithm = 'arpack')),('norm',Normalizer(copy=False)),RandomForestClassifier()])\n",
    "    \n",
    "    elif 'gloveWiki' in classifier:\n",
    "        if not idf:\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(gloveW2VWiki)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else: #'tflove' in classifier: #glove with tfidf\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VWiki)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    \n",
    "    elif 'gloveTwitter' in classifier:\n",
    "        if not idf:\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(gloveW2VTwitter)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else: #'tflove' in classifier: #glove with tfidf\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VTwitter)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    \n",
    "    elif 'gloveCommonCrawlBig' in classifier:\n",
    "        if not idf:\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(gloveW2VCommonCrawlBig)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else: #'tflove' in classifier: #glove with tfidf\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VCommonCrawlBig)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    \n",
    "    elif 'gloveCommonCrawlSmall' in classifier:\n",
    "        if not idf:\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(gloveW2VCommonCrawlSmall)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else: #'tflove' in classifier: #glove with tfidf\n",
    "            clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(gloveW2VCommonCrawlSmall)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    \n",
    "    elif 'cbow' in classifier:\n",
    "        if ngram==1:\n",
    "            if not idf:\n",
    "                clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(CBOW_Unigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "            else:\n",
    "                clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(CBOW_Unigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else:\n",
    "            if not idf:\n",
    "                 clf = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(CBOW_Bigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "            else:#'tfbigram' in classifier:\n",
    "                 clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(CBOW_Bigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    elif 'skipgram' in classifier:\n",
    "        if ngram==1: \n",
    "             clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(skipgrams_Unigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "        else: \n",
    "             clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(skipgrams_Bigram_W2V)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "    \n",
    "    elif 'skipRandom' in classifier:\n",
    "        if ngram==1: \n",
    "             clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(skipgrams_Unigram_W2V)),('rf', RandomForestClassifier())])\n",
    "        else: \n",
    "             clf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(skipgrams_Bigram_W2V)),('rf', RandomForestClassifier())])\n",
    "    \n",
    "    elif 'adaBoost' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('sgd', AdaBoostClassifier())])\n",
    "    \n",
    "    elif 'sgd' in classifier:\n",
    "        clf = Pipeline([('vect', vectorizer), ('sgd', SGDClassifier(class_weight=\"auto\"))])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,stratify=y)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    scores(y_test,y_predicted)\n",
    "    stop_time = time.time()\n",
    "    print(\"Running Time is %.2fseconds\" % (stop_time - start_time))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agressive_tweet_classed.txt',sep='==>',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = cleanTweets(df['tweet'].tolist())\n",
    "labels = df['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsTiny = tweets[:2000]\n",
    "tweetsSmall = tweets[:8000]\n",
    "\n",
    "labelsTiny = labels[:2000]\n",
    "labelsSmall = labels[:8000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_profane_length = find_max_length()\n",
    "max_profane_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"skyblue\">Data Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the chart\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "word_vectors = [tweet_w2v[w] for w in tweet_w2v.wv.vocab.keys()[:5000]]\n",
    "\n",
    "# dimensionality reduction. converting the vectors to 2d vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "# putting everything in a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "tsne_df['words'] = tweet_w2v.wv.vocab.keys()[:5000]\n",
    "\n",
    "# plotting. the corresponding word appears when you hover on the data point.\n",
    "plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"bieg\"> Codec conversion to handle tweets </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Female Killer In Las Vegas Shouted \\x81\\xe6\\xe4\\xf3\\xee\\xe4\\xf3_Allahu Akbar\\x81\\xe6\\xe4\\xf3\\xee\\xed\\x9a As She Ran Over 40 Innocent People Last Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode(text,errors='ignore').encode(\"latin-1\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonUtfIds[1],unicode(texts[nonUtfIds[1]],errors='ignore').encode(\"latin-1\",\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Traditional Machine Learning </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('logistic',ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('randomforest',ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('randomforest',X = tweetsSmall, y=labelsSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('randomforest',X = tweetsTiny, y=labelsTiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveWiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveWiki',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveTwitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveTwitter',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveCommonCrawlBig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveCommonCrawlBig',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveCommonCrawlSmall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('gloveCommonCrawlSmall',idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('cbow',ngram =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('cbow',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('cbow',ngram =2, idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec: Skipgrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('skipgrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('skipgrams',ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('skipgrams',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('skipgrams',ngram=2, idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('bernoulliTfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('bernoulliGlove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('3nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('5nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('extraTrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('extraTrees',ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('extraTrees',X = tweetsSmall, y=labelsSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('extraTrees',X = tweetsTiny, y=labelsTiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('svcGlove',idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\"> Model Evaluation: Misclassified & Confusion Matrix</font>\n",
    "http://www.ritchieng.com/machine-learning-evaluate-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', min_df=3,strip_accents='unicode', analyzer ='word', tokenizer=LemmaTokenizer(), ngram_range=(1, 2), use_idf=True, sublinear_tf=1, stop_words = \"english\")\n",
    "logistic_model = Pipeline([('vect', vectorizer), ('logistic', LogisticRegression(solver='newton-cg'))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.2)\n",
    "logistic_model.fit(X_train,y_train)\n",
    "y_predicted = logistic_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countFP,countFN = 0,0\n",
    "FP, FN = [],[]\n",
    "for i in range(len(y_predicted)):\n",
    "    if y_predicted[i] != y_test[i]:\n",
    "        if y_test[i] == 0 and y_predicted[i] == 1:\n",
    "            FP += [X_test[i]]\n",
    "            countFP += 1\n",
    "        else:\n",
    "            FN += [X_test[i]]\n",
    "            countFN += 1\n",
    "            \n",
    "countFP, countFN, len(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='bieg'> Examples that are misclassified as offensive </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\"> Examples that are misclassified as non-offensive </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Deep Neural Networks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"brown\">LSTMs without Dropout</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_profane_length = 174\n",
    "embedding_vector_length = 32\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.2)\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_profane_length)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_profane_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "optimizer = optim(lr=0.001)\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_profane_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> LSTMs with Dropout </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"darkblue\">Batch size: 32</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"olive\">Batch size: 64</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR = 10^-3 (with earlystopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_patience, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with little patience (patience = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_littlepatience,shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with little patience = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.0005), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_littlepatience,shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.0002), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_littlepatience,shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with patience and lr=2x10^-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.0002), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_patience,shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR = 10^-4 (with  earlystopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.0001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, validation_split=0.1, callbacks = earlystopping_patience, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR = 10^-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(lr=0.01), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"RoyalBlue\">Reversed tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversedTweets = []\n",
    "for tweet in tweets:\n",
    "    reversedTweets += [reverseText(tweet)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size : 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=reversedTweets)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, callbacks=earlystopping_nopatience,verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=reversedTweets)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, callbacks=earlystopping_nopatience,verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=reversedTweets)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, callbacks=earlystopping_nopatience,verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Dataset (Size = 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=tweetsSmall,y=labelsSmall)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=tweetsSmall,y=labelsSmall)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiny Dataset (Size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=tweetsTiny,y=labelsTiny)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X=tweetsTiny,y=labelsTiny)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, callbacks =earlystopping_patience, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1, callbacks =earlystopping_patience, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_len, trainable=True ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1,callbacks=earlystopping, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_len, trainable=True ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1,callbacks=earlystopping, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"royalblue\">Two Layered LSTMs </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length, trainable=True ))\n",
    "model.add(LSTM(32,return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, callbacks=earlystopping, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> LSTMs with glove - Twitter </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, callbacks=earlystopping, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 200,  weights=[glove_twitterBig_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, callbacks=earlystopping, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"maroon\"> LSTMs with twitter word2vec </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(cbow_word_model, CBOW_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(CBOW_word_W2V[CBOW_word_W2V.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgram_word_model, skipgram_word_W2V, tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgram_word_W2V[skipgram_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgram_phrase_model, skipgram_phrase_W2V, tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgram_word_W2V[skipgram_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/\n",
    "2. https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "3. https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More resources:\n",
    "1. https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "2. https://machinelearningmastery.com/improve-deep-learning-performance/\n",
    "3. http://www.ritchieng.com/machinelearning-learning-curve/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> GRUs </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"purple\"> LSTMs with CNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=5000\n",
    "embedding_vecor_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(cbow_word_model, CBOW_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(CBOW_word_W2V[CBOW_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"brown\">Batch size : 32</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=40, verbose=1, validation_split=0.1, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"magenta\">Batch size:64</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=40, verbose=1, validation_split=0.1, batch_size=64,callbacks=earlystopping_patience)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='bieg'>Batch Size 128 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=40,verbose=1,validation_split=0.1, batch_size=128,callbacks=earlystopping_patience)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Bidirectional LSTMs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"bieg\">Fast Text </font>\n",
    "https://martinbel.github.io/fast-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim(optimizer='adam',lr=0.001):\n",
    "    if optimizer == 'adam':\n",
    "        optim = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optim = RMSprop(lr=lr, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    elif optimizer == 'sgd-momentum':\n",
    "        optim = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif optimizer == 'adagrad':\n",
    "        optim = Adagrad(lr=lr, epsilon=1e-08, decay=0.0)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,  input_length=max_profane_length))\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optim(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "predicted = model.predict_classes(X_test)\n",
    "scores(y_test,predicted)\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Skipgram (Negative Sampling) with FastText</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgramNeg_word_model = Word2Vec(unigram_transformer, sg=1, size=100,negative=20)\n",
    "skipgramNeg_word_W2V = dict(zip(skipgramNeg_word_model.wv.index2word, skipgramNeg_word_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgramNeg_word_model, skipgramNeg_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgramNeg_word_W2V[skipgramNeg_word_W2V.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# print 'Fast Text Summary'\n",
    "# model.summary()\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optim(), metrics=['accuracy'])\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"purple\">FastText with Twitter GloVe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print 'Fast Text Summary'\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
