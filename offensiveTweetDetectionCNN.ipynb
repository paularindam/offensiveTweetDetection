{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/best-practices-document-classification-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "from gensim.models import Phrases, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import word_tokenize,pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense,Activation, Flatten, LSTM,Convolution1D,Dropout\n",
    "\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence,text as Text\n",
    "from keras.utils import vis_utils #import plot_model, model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "google_pretrained_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(texts):\n",
    "    cleanedTexts = []\n",
    "    for text in texts:\n",
    "        cleanedTexts += [unicode(str(text),errors='ignore').encode(\"latin-1\",\"ignore\").replace('\\r','')]\n",
    "    return cleanedTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('agressive_tweet_classed.txt',sep='==>',encoding='utf-8')\n",
    "\n",
    "tweets = cleanTweets(df['tweet'].tolist())\n",
    "labels = df['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model):\n",
    "    return SVG(vis_utils.model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "def generateEmbeddingMatrix(word_vectors, embedding, word_index, vocabulary_size=vocab_size):\n",
    "    NUM_WORDS = vocabulary_size\n",
    "    embedding_dim = len(embedding[embedding.keys()[0]])\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i>=NUM_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"bieg\">Preparing Google PreTrained Embeddings</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "NUM_WORDS = vocab_size\n",
    "google_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        google_embedding_vector = google_pretrained_vectors[word]\n",
    "        google_embedding_matrix[i] = google_embedding_vector\n",
    "    except KeyError:\n",
    "        google_embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(google_pretrained_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\"> Twitter Glove Embedding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for line in open('glove/glove.twitter.27B.50d.txt'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "glove_twitter_embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_twitter_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for line in open('glove/glove.twitter.27B.200d.txt'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "glove_twitterBig_embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_twitterBig_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTestTrainCurves(history):\n",
    "    print \"History Summary for accuracy\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    print \"History Summary for loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X=tweets,y=labels):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    ###################################\n",
    "    X,y = shuffle(X,y,random_state=10)\n",
    "    X,y = shuffle(X,y,random_state=0)\n",
    "    ###################################\n",
    "    tk = Text.Tokenizer(lower=True)\n",
    "    tk.fit_on_texts(X)\n",
    "\n",
    "    x = tk.texts_to_sequences(X)\n",
    "\n",
    "    ###################################\n",
    "    max_len = 174\n",
    "    embedding_vecor_length = 32\n",
    "    print \"max_len \", max_len\n",
    "    print('Pad sequences (samples x time)')\n",
    "\n",
    "    x = sequence.pad_sequences(x, maxlen=max_len)\n",
    "\n",
    "    return model_selection.train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "def scores(y_test,y_pred):\n",
    "    print \n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test,y_pred)*100))\n",
    "    print(\"Precision: %.2f%%\" % (precision_score(y_test,y_pred)*100))\n",
    "    print(\"Recall: %.2f%%\" % (recall_score(y_test,y_pred)*100))\n",
    "    print(\"F1-score: %.2f%%\" % (f1_score(y_test,y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(texts=tweets):\n",
    "    maxLength = -1\n",
    "    for text in texts:\n",
    "        if len(text)>maxLength:\n",
    "            maxLength = len(text)\n",
    "    return maxLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Words(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        tokens =  word_tokenize(sentence)\n",
    "        words +=[tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetCorpus = cleanTweets(open('profane_tweets.txt').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_transformer = Words(tweetCorpus)\n",
    "phrase2vec_transformer = Phrases(tweetCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_word_model = Word2Vec(word2vec_transformer, size=100)\n",
    "CBOW_word_W2V = dict(zip(cbow_word_model.wv.index2word, cbow_word_model.wv.syn0))\n",
    "\n",
    "skipgram_word_model = Word2Vec(word2vec_transformer, sg=1, size=100)\n",
    "skipgram_word_W2V = dict(zip(skipgram_word_model.wv.index2word, skipgram_word_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_profane_length = find_max_length()\n",
    "max_profane_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.count(0), labels.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN model without Maxpooling\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_profane_length))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_profane_length))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1, validation_split=0.1, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(cbow_word_model, CBOW_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(CBOW_word_W2V[CBOW_word_W2V.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_nopatience = [EarlyStopping(monitor='val_loss')]\n",
    "earlystopping_patience = [EarlyStopping(monitor='val_loss',patience=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(cbow_word_model, CBOW_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(CBOW_word_W2V[CBOW_word_W2V.keys()[0]])\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Google Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = google_embedding_matrix\n",
    "embedding_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks = earlystopping, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = google_embedding_matrix\n",
    "embedding_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=False))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks = earlystopping_patience, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "predicted = model.predict_classes(X_test)\n",
    "scores(y_test,predicted)\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = google_embedding_matrix\n",
    "embedding_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(512, 3, border_mode='same'))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks = earlystopping, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams with negative sampling (negative = 20 ~ default = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgramNeg_word_model = Word2Vec(word2vec_transformer, sg=1, size=100,negative=20)\n",
    "skipgramNeg_word_W2V = dict(zip(skipgramNeg_word_model.wv.index2word, skipgramNeg_word_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgramNeg_word_model, skipgramNeg_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgram_word_W2V[skipgram_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgramNeg_word_model, skipgramNeg_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgram_word_W2V[skipgram_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = generateEmbeddingMatrix(skipgramNeg_word_model, skipgramNeg_word_W2V,tokenizer.word_index, vocabulary_size=vocab_size)\n",
    "embedding_dim = len(skipgram_word_W2V[skipgram_word_W2V.keys()[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_phrase_model = Word2Vec(phrase2vec_transformer[tweetCorpus], sg=1, size=100, negative=20)\n",
    "skipgram_phrase_W2V = dict(zip(skipgram_phrase_model.wv.index2word, skipgram_phrase_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 5, border_mode='same'))\n",
    "model.add(Convolution1D(32, 5, border_mode='same'))\n",
    "model.add(Convolution1D(16, 5, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 5, border_mode='same'))\n",
    "model.add(Convolution1D(32, 5, border_mode='same'))\n",
    "model.add(Convolution1D(16, 5, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 5, border_mode='same'))\n",
    "model.add(Convolution1D(32, 5, border_mode='same'))\n",
    "model.add(Convolution1D(16, 5, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.1, callbacks=earlystopping, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"garden\">Single Layer CNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2048,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"inkblue\">Twitter Glove with CNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50,  weights=[glove_twitter_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 200,  weights=[glove_twitterBig_embedding_matrix], input_length=max_profane_length,trainable=True))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.1, shuffle=True,callbacks=earlystopping)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to tinker with \n",
    "1. How to decide on how much dense it should be \n",
    "2. Reduce dropout before dense layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN model with Maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_profane_length))\n",
    "model.add(Convolution1D(256, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Convolution1D(128, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Convolution1D(32, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, callbacks = earlystopping, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_profane_length))\n",
    "model.add(Convolution1D(256, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Convolution1D(128, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Convolution1D(64, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Convolution1D(16, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, callbacks = earlystopping, validation_split=0.1, shuffle=True)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "plotTestTrainCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://phontron.com/class/nn4nlp2017/assets/slides/nn4nlp-05-cnn.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolutional Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "model.add(Convolution2D(64, 3, border_mode='same'))\n",
    "model.add(Convolution2D(32, 3, border_mode='same'))\n",
    "model.add(Convolution2D(16, 3, border_mode='same'))\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
    "         callbacks=callbacks)  # starts training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
